{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, color\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the directory path\n",
    "train_directory_path = \"/Users/Admin/Documents/HKU Fintech Work/Year 2/Year 2 Sem 2/COMP3314 Machine Learning/Group Project/COMP3314-Group-Project/train_ims\"\n",
    "test_directory_path = \"/Users/Admin/Documents/HKU Fintech Work/Year 2/Year 2 Sem 2/COMP3314 Machine Learning/Group Project/COMP3314-Group-Project/test_ims\"\n",
    "\n",
    "# List filenames in alphabetical order\n",
    "train_filenames = sorted(os.listdir(train_directory_path))\n",
    "test_filenames  = sorted(os.listdir(test_directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # Flip horizontally with 50% probability\n",
    "    iaa.Flipud(0.5),  # Flip vertically with 50% probability\n",
    "    iaa.Affine(rotate=(-10, 10)),  # Randomly rotate the image by up to 10 degrees\n",
    "    iaa.GaussianBlur(sigma=(0.0, 0.5)),  # Apply Gaussian blur with a sigma between 0 and 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/Users/Admin/Documents/HKU Fintech Work/Year 2/Year 2 Sem 2/COMP3314 Machine Learning/Group Project/COMP3314-Group-Project/train.csv\") #change path\n",
    "test_df = pd.read_csv(\"/Users/Admin/Documents/HKU Fintech Work/Year 2/Year 2 Sem 2/COMP3314 Machine Learning/Group Project/COMP3314-Group-Project/test.csv\") #change path\n",
    "\n",
    "\n",
    "for i in range(50000): #only checking first 1000 images\n",
    "    image = io.imread(f\"{train_directory_path}/{train_filenames[i]}\")\n",
    "    x_train.append(image)\n",
    "\n",
    "    '''if i < 40000:\n",
    "        x_train.append(image)\n",
    "    else:\n",
    "        x_test.append(image)\n",
    "    '''\n",
    "augmented_images = aug(images=x_train[10000:25000])\n",
    "\n",
    "'''\n",
    "for image in x_train[30000:40000]:\n",
    "    augmented_images = augmentations(images=[image])\n",
    "    augmented_x_train = np.concatenate((augmented_x_train, augmented_images), axis=0)\n",
    "'''\n",
    "\n",
    "for j in range(10000): #only checking first 500 images\n",
    "    image = io.imread(f\"{test_directory_path}/{test_filenames[j]}\")\n",
    "    x_test.append(image)\n",
    "\n",
    "'''\n",
    "y_train = np.concatenate((np.array(train_df[\"label\"].to_numpy()[:40000]), np.array(train_df[\"label\"].to_numpy()[30000:40000])))\n",
    "y_test = np.array(train_df[\"label\"].to_numpy()[-10000:])\n",
    "\n",
    "x_train = np.concatenate((np.array(x_train), augmented_x_train))\n",
    "x_test = np.array(x_test)\n",
    "'''\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_train = np.concatenate((x_train, augmented_images))\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_train = np.array(train_df[\"label\"].to_numpy())\n",
    "y_train = np.concatenate((y_train, y_train[10000:25000]))\n",
    "\n",
    "y_test = np.array(test_df[\"label\"].to_numpy())\n",
    "\n",
    "assert x_train.shape == (65000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "assert y_train.shape == (65000,)\n",
    "assert y_test.shape == (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sift_feature(im):\n",
    "    gray_im = color.rgb2gray(im)\n",
    "    sift = cv2.SIFT_create()\n",
    "    _, descriptors = sift.detectAndCompute((gray_im * 255).astype(\"uint8\"), None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros((128,))\n",
    "    return np.mean(descriptors, axis=0) \n",
    "\n",
    "def get_hog_feature(im):\n",
    "    gray_im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) / 255\n",
    "    hog_features = hog(gray_im, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "\n",
    "    hsv_image = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    hist_features = []\n",
    "    for channel in cv2.split(hsv_image):\n",
    "        hist_features.append(cv2.calcHist([channel], [0], None, [8], [0, 256]))\n",
    "\n",
    "    hist_features = np.concatenate(hist_features).flatten() / (image.shape[0] * image.shape[1])\n",
    "\n",
    "    merged_features = np.concatenate((hog_features, hist_features))\n",
    "    return(merged_features)\n",
    "\n",
    "\n",
    "def sift_hog_combined_features(im):\n",
    "    hog_features = get_hog_feature(im)\n",
    "    sift_features = get_sift_feature(im)\n",
    "    combined_features = np.concatenate((hog_features, sift_features))\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_hog_features_train = []\n",
    "sift_hog_features_test = []\n",
    "\n",
    "for image1 in x_train:\n",
    "    sift_hog_features_train.append(sift_hog_combined_features(image1)) #checking for sift feature \n",
    "\n",
    "for image2 in x_test:\n",
    "    sift_hog_features_test.append(sift_hog_combined_features(image2)) #checking for sift feature \n",
    "\n",
    "sift_hog_features_train = np.array(sift_hog_features_train)\n",
    "sift_hog_features_test = np.array(sift_hog_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "pca1 = PCA(0.9)\n",
    "pca1.fit(sift_hog_features_train)\n",
    "print(pca1.n_components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_internal_train, X_internal_test, y_internal_train, y_internal_test = train_test_split(sift_hog_features_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "pipe_svm = Pipeline([('StandardScaler', MinMaxScaler()), ('pca', PCA(0.9)), ('rbfsvc', SVC(kernel='rbf', C=5, gamma = 0.5, cache_size=10000))])\n",
    "\n",
    "\n",
    "pipe_svm.fit(X_internal_train, y_internal_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#get accuracy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpipe_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_internal_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_internal_test, y_pred)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/pipeline.py:480\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    478\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:506\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m        Transformed data.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    509\u001b[0m         X,\n\u001b[1;32m    510\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    516\u001b[0m     X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#get accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = pipe_svm.predict(X_internal_test)\n",
    "accuracy = accuracy_score(y_internal_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "color = 'white'\n",
    "cm = confusion_matrix(y_internal_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get csv file\n",
    "y_pred = voting_clf.predict(sift_hog_features_test)\n",
    "\n",
    "\n",
    "#test_df[\"label\"] = y_pred\n",
    "#test_df.to_csv(\"/Users/Admin/Documents/HKU Fintech Work/Year 2/Year 2 Sem 2/COMP3314 Machine Learning/Group Project/COMP3314-Group-Project/rishi_work/rishi_test4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
